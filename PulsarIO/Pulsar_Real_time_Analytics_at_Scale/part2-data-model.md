第二部分：数据和处理模型
================================


2.1 用户行为数据
----------------------
用户行为数据通常都是非结构化或半结构化的，它由许多信息元组组成。每个元组代表一个维度，用户行为数据流是由连续的事件组成，每个事件上包含了点击的上下文，同时它由很多维度（超过40个）组成。例如，pageID,IP地址，地理坐标信息如城市，地区和国家，设备标识数据如浏览器类型，设备类型以及更多的维度。

这些维度的数据的基数都很大，使数据变得稀疏的。其中一个具有挑战性就是在海量的数据中实时地做计数器的聚合计算，在实时计算环境中处理空间和时间是个非常有挑战性的问题。  

通常我们看到用户行为数据流每秒处理数据百万事件，事件产生在不可预知的模式中，操作这些数据的场景需求非常低的延迟，这需要内存中处理数据流就像数据是流过系统一样。

2.2 混合数据
----------------------
用户行为数据流包括BOT和用户行为活动（user behavior activity）。我们终端用户只对用户行为活动感觉兴趣，这要求BOT活动事件能被够检测并过虑, 其他的场景只是处理BOT事件然后用户活动事件需要被过滤。

经常其他来源的数据而且非常有价值的信息需要与用户行为流结合，地理信息，设备标识，人口统计资料和某些片段等这些数据。挑战在于设计可伸缩的数据存储，查询速度也需要讨论。许多场景要求这种类型的数据，访问这些数据源对于独立应用服务来说是非常昂贵的，而更为有效的方法是提供能描述这个数据流的动态维度，这涉及到找一个使用在事件上单个维度作为键的存储。我们使用的策略是在处理节点做缓存或者使用一个快速查询的缓存。这个讨论必需基于数据改变的频率和数据的大小，这类似数据库的连接操作(`join`)

2.3 Session化
-----------------------
Session化是通过一个共同键来识别相关事件分组的状态管理流程。在访问分析中，访问或者会话被定义为一系列的页面请求，某个标签，相同客户端的图片请求。访问在指定的时间里没有请求记录就会被认为完成了，许多分析工具都使用30分钟做限制（『超时』），在一些工具可以改变这个值，会话状态通常包含一个或多个某些行为的计数器（例如，页面导航的记录）。   

我们的解决方法是设计一个租户自定义会话，租户是一个会话化数据的消费者。每个租户的会话有着自己的生命周期。租户的会话就是由一个或多事件维度的组成的唯一标识所定义，会话标识，持久，元数据和状态管理逻辑都可以通过声明式的语法来指定。

2.4 流的自定义视图
--------------------------
通常地，事件来源于一个有着大量维度的大数据应用。在这通过广域网的分布式通道来传输这大事件是非常昂贵的。很多用户通常只关心原始流的部分视图。如果直接消费原始流，这个流的消费者就需要花费大量投入才能得到原始流的部分视图。基于容量的订阅需要给用户提供声明式定义原始流不同视图。用户可以在原始流中根据他们感兴趣的地方来定义维度，然后通过过滤规则来填充视图。数据的填充需要根据订阅者的授权等级来作限定。

2.5 实时多维度聚合计算
---------------------------
聚合就是根据一组维度做汇总数据。关系型数据库或者PIG或者HIVE提供的聚合功能有COUNT，SUM，MIN，MAX，AVG，DISTINCT COUNT，TOP N，QUANTILES等。

看一下在关系型数据库世界的语句：`select count(*) as METRIC1, column1, column2 from SOMETABLE group by column1, column2` 。这语句在一个叫SOMETABLE的表来计算column1和column2的总数。

这个查询运行在一个单独的数据库实例上，如果在共享环境，一个应用可能需要运行在每个分片上然后通过每个分片上的结果再进行聚合，如果被扫描的行数很大这个查询就会执行很长时间。

现在考虑一下在实时流环境中相似的查询。事件中的维度代替列和事件流代替数据库表。在章节2.1我们提到电商系统通常每秒要处理数据百万条事件。跟数据库不同，实时数据流是持续地处理数据的，我们需要给持续处理的聚合功能提供一个开始时间和结束时间。我们的实现方式是每个聚合执行定义一个窗口[注1]，这个窗口是滚动窗口（`Tumbing widows`）或者旋转窗口(`Rolling windows`)。
下面一个平常类SQL语句展示了使用数据流实现在时间窗口为10秒以D1和D2分组的汇总查询:

create context MCContext start @now end pattern [timer:interval(10)]  

context MCContext    
insert into AGGREGATE select count(*) as METRIC1, D1, D2, FROM RAWSTREAM group by D1,D2 output snapshot wher terminated;     

select * from AGGERGATE;

由于实时流处理都是在内存中所以我们受到内存空间的限制。由于流式数据的是按某个范围计算，大量数据统计会对内存资源造成很大的压力,因此我们建议把时间窗口设小（10秒）,我们建议使用一个分区的策略来实现集群中的计算节点作负载均衡，事件通过一致性哈稀算法[注2]来分配到各个节点，这算法通过一个在逻辑环的哈稀键来查找事件需要被分配的节点。这哈稀键通过一个或多个事件维度来计算一个组合键的哈稀值(H(D1,D2,...))。我们建议使用一个128 bit的哈稀函数，这样我们就能得一个优雅的跨集群的事件。

通过聚合产生的数据就是时间序列数据。由于全部数据都在内存中计算，所以如果节点宕机会导致数据丢失。我们的策略是减小聚合的时间窗（30秒到1分钟），当时间窗口中滚动，数据通过计算引擎就像指标数据一样分发出去，我们把这些事件存储在时间序列数据库，我们就能够创建一个更长时间窗口（1小时或1天）的聚合计算。这个快照式的方法有点像journal[注3]- 我们能够存储快照数据在一个时序数据库然后从不同计算结点中还原。因为大多数我们的用例是统计报表，所我们把这些数据放在一起丢失也会很小。

我们还用快照(`snapshots`)来实现实时可视化插件，这些插件放在实时仪表盘上，展示可视化数据。几个实时可视化以这种方式发布我们的生产环境发布。

这种聚合方法通过已知的维度来产生新的维度，在这种情况下维度组都是预知的，这非常合适实时计算的场景。当数据存储在时间序列数据它就能使用报表场景，但是这方法它本身并不适合数据探索。数据探索涉及执行特定的查询，如创建以许多随机维度建分组汇总。本文4.5节中,我们将探索一种方法结合online与offline和如何利用pre-aggregation去提高时间序列数据库的查询性能。

2.6 实时Top N,Precetitles和Distinct Count指标计算
---------------------------
考虑以下查询：以1分钟为时间窗查找以D1，D2，和D3维度作为分组汇总后取数量最大的10个。

create context MCContext start @now end pattern [timer:interval(60)]

context MCContext

insert into TOPITEMS select count(1) from RAWEventStream group by D1, D2, D3 order by count(1) limit 10;


这个查询的执行成本与D1，D2，D3组合维度和事件进入时间窗口的速率成正比。如果一个或多个维度的数据量很大，大量的计数器计算就会占用大量的内存资源。`ORDER BY` 指令当时间窗口滚动的时候会把数据作排序，这会消耗很多计算资源从而已导致系统就慢，我们系统是构建在Javba，这会导致频繁的GC，其他非常有用指标统计像Distinct Count和Percetitles也有类似的问题,因为他们是内存密集型。

然而，我们考虑使用近似算法去计算Top N，百分比和Distinct count查询，然后我们可以在一个可以控制固定的内存中操作，我们建议使用多次估算技术去计算Top N，在这个方法里我们只会对内存中的组合频繁操作。当存储满了，最常用的分组从内存中丢弃，然后成为一个新的计数。我们已经使用了一个特殊的聚合函数来实现这个算法，然而我们可以使用以下这个查询：

select TopN(1000, 10, D1) as topItems from RawEvent();

聚合函数每一个参数指的是容量，第二个参数相当返回的最大数据数（TopN），最后一个参数 是事件的维度。

我们建议使用HyperLogLog[注4]和TDigest[注5]算法来计算Distinct Count和Percentiles。我们提供这两种计算的聚合函数，我们可以在事件流中使用一个类SQL来实现。

通过这个方法我们可以能控制空间和时间来提高计算的精确性。以我们的经验，我们发现这些近似算法引入的误差大约1%取决于数据集和容量分配算法，这对于大多数的统计问题,在我们看来是可以接受的。

2.6 处理无序和廷迟事件
---------------------------
对于一个分布式系统来说实现有序是非常困难的，有时候由于网络问题，我们必须先把事件存储一个持久的队列里然后再回放，这就导致廷迟和无序。

移动设备通常每分钟批量地发送一批事件。这些事件通过中间队列传输，要比记录在移动设备慢很多才能到达系统。通常我们要求在设备在一个指定的时间窗计算出某些维度的分组汇总指标数据。我们希望我们的指标数据尽可能准确到分钟,小时或一天。

作为无序和廷迟事件的补偿措施，我们建议发送给我们系统的事件加上一个时间戳维度，在我们系统时间戳会四舍五入为最接近的分钟然后注入回事件。我建议所有指标数据都有四舍五入的时间戳作为其中一个维度。当事件廷时或者无序的时候，我可以使用时序数据库根据它正确地时间窗总汇。


-------


1. [Tumbing widows](https://developer.ibm.com/streamsdev/2014/05/06/spl-tumbling-windows-explained/) [epl:view](http://www.espertech.com/esper/release-5.2.0/esper-reference/html/epl-views.html#win-views) [introducing windows](https://flink.apache.org/news/2015/12/04/Introducing-windows.html)
2. [一致性哈稀]()
3. [journal]()
4. [HyperLogLog: the analysis of a near-optimal cardinality estimation algorithm](http://algo.inria.fr/flajolet/Publications/FlFuGaMe0 7.pdf)
5. [Computing extremely accurate quantiles using T- Digest](https://github.com/tdunning/t- digest/blob/master/docs/t-digest-paper/histo.pdf)
6. [directed acyclic graph (DAG)](https://en.wikipedia.org/wiki/Directed_acyclic_graph)

-----------------

[« 第一部分: 简介](part1-introduction.md)　　　　[第三部分: 架构 »](part3-architecture.md)
